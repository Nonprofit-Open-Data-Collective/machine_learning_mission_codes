region="metropolitan statistical area/micropolitan statistical area:*" )
View(dat.11)
dat.11  <- getCensus( name="acs5/profile",
vintage=2012,
key=censuskey,
vars="DP03_0009PE",
region="metropolitan statistical area/micropolitan statistical area:*" )
View(dat.11)
dat.11  <- getCensus( name="acs5/profile",
vintage=2011,
key=censuskey,
vars="DP03_0009PE",
region="metropolitan statistical area/micropolitan statistical area:*" )
library(maps)
x <- map("africa", plot=FALSE)
gatsby <- c("In my younger and more vulnerable years my father gave
me some advice that I’ve been turning over in my mind
ever since.
‘Whenever you feel like criticizing any one,’ he told me,
‘just remember that all the people in this world haven’t had
the advantages that you’ve had.’
He didn’t say any more but we’ve always been unusually
communicative in a reserved way, and I understood that he
meant a great deal more than that. In consequence I’m inclined to reserve all judgments, a habit that has opened up
many curious natures to me and also made me the victim
of not a few veteran bores. The abnormal mind is quick to
detect and attach itself to this quality when it appears in a
normal person, and so it came about that in college I was
unjustly accused of being a politician, because I was privy
to the secret griefs of wild, unknown men. Most of the confidences were unsought—frequently I have feigned sleep,
preoccupation, or a hostile levity when I realized by some
unmistakable sign that an intimate revelation was quivering on the horizon—for the intimate revelations of young
men or at least the terms in which they express them are
usually plagiaristic and marred by obvious suppressions.
Reserving judgments is a matter of infinite hope. I am still
a little afraid of missing something if I forget that,
as my father snobbishly suggested, and I snobbishly repeat a sense
of the fundamental decencies is parcelled out unequally at
birth.")
gatsby <- "In my younger and more vulnerable years my father gave
me some advice that I’ve been turning over in my mind
ever since.
‘Whenever you feel like criticizing any one,’ he told me,
‘just remember that all the people in this world haven’t had
the advantages that you’ve had.’
He didn’t say any more but we’ve always been unusually
communicative in a reserved way, and I understood that he
meant a great deal more than that. In consequence I’m inclined to reserve all judgments, a habit that has opened up
many curious natures to me and also made me the victim
of not a few veteran bores. The abnormal mind is quick to
detect and attach itself to this quality when it appears in a
normal person, and so it came about that in college I was
unjustly accused of being a politician, because I was privy
to the secret griefs of wild, unknown men. Most of the confidences were unsought—frequently I have feigned sleep,
preoccupation, or a hostile levity when I realized by some
unmistakable sign that an intimate revelation was quivering on the horizon—for the intimate revelations of young
men or at least the terms in which they express them are
usually plagiaristic and marred by obvious suppressions.
Reserving judgments is a matter of infinite hope. I am still
a little afraid of missing something if I forget that,
as my father snobbishly suggested, and I snobbishly repeat a sense
of the fundamental decencies is parcelled out unequally at
birth."
gatsby <- tolower(gatsby)
library(stringer)
my_string_vector <- str_split(gatsby, "!")[[1]]
install.packages("stringer")
install.packages("stringr")
my_string_vector <- str_split(gatsby, "!")[[1]]
library(stringr)
my_string_vector <- str_split(gatsby, "!")[[1]]
my_string_vector <- str_split(gatsby, c("!", ".")[[1]]
my_string_vector <- str_split(gatsby, c("!", "."))[[1]]
View(my_string_vector)
my_string_vector <- str_split(gatsby, c( "."))
View(my_string_vector)
my_string_vector <- str_split(gatsby, "!")[[1]]
library(quanteda)
install.packages("quanteda")
library(quanteda)
tokens(gatsby)
tokens(gatsby, remove_numbers = TRUE,  remove_punct = TRUE)
?Maxent_Sent_Token_Annotato
??openNLP
install.packages("openNLP")
?Maxent_Sent_Token_Annotato
?Maxent_Sent_Token_Annotaton
library(openNLP)
?Maxent_Sent_Token_Annotaton
?Maxent_Sent_Token_Annotato
Maxent_Sent_Token_Annotator(gatsby)
annotate(gatsby, Maxent_Sent_Token_Annotator(probs=TRUE)
)
install.packages("qdap")
library(qdap)
sent_detect_nlp(gatsby)
library(qdap)
library(qdap)
library(qdap)
??qdap
install.packages(qdap)
install.packages('qdap')
library(qdap)
install.packages('corpus')
text_tokens(gatsby, stemmer = "en")
library(corpus)
text_tokens(gatsby, stemmer = "en")
gatsby
topfeatures(gatsby, 5)
install.packages("quanteda")
library(quanteda)
speeches <- quanteda.corpora::data_corpus_sotu
install.packages("quanteda.corpora")
install.packages("devtools")
devtools::install_github("quanteda/quanteda.corpora")
install.packages("quanteda.corpora")
devtools::install_github("quanteda/quanteda.corpora")
library(devtools)
devtools::install_github("quanteda/quanteda.corpora")
devtools::install_github("quanteda/quanteda.corpora")
speeches <- quanteda.corpora::data_corpus_sotu
library(corpus)
my_corpus1 <- corpus(data_corpus_inaugural[1:5])
my_corpus2 <- corpus(data_corpus_inaugural[53:58])
my_corpus3 <- my_corpus1 + my_corpus2
summary(my_corpus3)
my_corpus <- corpus(data_corpus_inaugural)
kwic(my_corpus, pattern = "terror")
tokens(my_corpus1)
my_corpus1.1 <- tolower(my.corpus1)
my_corpus1.1 <- tolower(my_corpus)
my_corpus1.1 <- tolower(my_corpus1)
tolower(my_corpus1)
my_corpus1.2 <- tokens(my_corpus1.1)
tokens(my_corpus1.1)
??tokens
my_corpus1.2 <- tokens(my_corpus1.1, remove_punct = TRUE)
myStemMat <- dfm(my_corpus1, remove = stopwords("english"), stem = TRUE, remove_punct = TRUE)
myStemMat
head(stopwords("en"))
head(stopwords("en"), 20)
head(stopwords("en"), 100)
topfeatures(my_dfm, 20)
write.csv(tocode2, "All Data to Code.csv")
topfeatures(myStemMat, 20)
myStemMat
??dfm
library(quanteda)
library(corpus)
knitr::opts_chunk$set(echo = FALSE)
summary(cars)
plot(pressure)
my_corpus <- corpus(data_corpus_inaugural)
kwic(my_corpus, pattern = "terror")
kwic(my_corpus, pattern = "dog")
kwic(my_corpus, pattern = "United States")
kwic(my_corpus, pattern = "love")
kwic(my_corpus, pattern = "violence")
kwic(my_corpus, pattern = "foe")
my_corpus <- corpus(data_corpus_inaugural[32:37])
kwic(my_corpus, pattern = "violence")
my_corpus <- corpus(data_corpus_inaugural[20:37])
kwic(my_corpus, pattern = "foe")
my_corpus <- corpus(data_corpus_inaugural[36:41])
kwic(my_corpus, pattern = "violence")
kwic(my_corpus, pattern = "foe")
my_corpus <- corpus(data_corpus_inaugural[36:43])
kwic(my_corpus, pattern = "foe")
my_corpus <- corpus(data_corpus_inaugural[36:44])
kwic(my_corpus, pattern = "foe")
head(tolower(my_corpus))
tolower(my_corpus)[20]
tolower(my_corpus)[,20]
my_corpus <- corpus(data_corpus_inaugural[42:44])
kwic(my_corpus, pattern = "foe")
kwic(my_corpus, pattern = "torch")
kwic(my_corpus, pattern = "fight")
tolower(my_corpus)
my_corpus1 <- tolower(my_corpus)
tokens(my_corpus1, remove_punct = TRUE, padding = TRUE)
my_corpus2 <- tokens(my_corpus1, remove_punct = TRUE, padding = TRUE)
my_corpus2 <- tokens(my_corpus1, remove_punct = TRUE)
my_corpus3 <- tokens_remove(tokens(my_corpus1.3), stopwords("english"), padding  = TRUE)
my_corpus3 <- tokens_remove(tokens(my_corpus2), stopwords("english"), padding  = TRUE)
tokens_remove(tokens(my_corpus2), stopwords("english"), padding  = TRUE)
ngram(my_corpus1.3[1], n=2)
my_corpus <- corpus(data_corpus_inaugural)
my_dfm <- dfm(my_corpus)
my_dfm[, 1:5]
my_dfm
dfm(my_corpus)
my_dfm2 <- dfm(my_corpus, remove = stopwords("english"), stem = TRUE, remove_punct = TRUE)
my_dfm2
rm()
rm(list = ls())
my_corpus <- corpus(data_corpus_inaugural, Year > 1990)
my_corpus <- corpus(data_corpus_inaugural, year > 1990)
corpus(data_corpus_inaugural[1:5])
my_corpus <- corpus(data_corpus_inaugural)
my_corpus <- subset(my_corpus, Year > 1990)
data_corpus_inaugural
names(data_corpus_inaugural)
my_corpus <- corpus(data_corpus_inaugural[1:5])
my_dfm <- dfm(my_corpus)
my_dfm
my_dfm2 <- dfm(my_corpus, remove = stopwords("english"), stem = TRUE, remove_punct = TRUE)
View(my_dfm2)
my_dfm[, 1:5]
my_dfm2[, 1:5]
head(stopwords("en"), 20)
topfeatures(my_dfm, 20)  # 20 top words
topfeatures(my_dfm2, 20)  # 20 top words
recent_corpus <- corpus_subset(data_corpus_inaugural, Year > 1991)
my_dict <- dictionary(list(terror = c("terrorism", "terrorists", "threat"),
economy = c("jobs", "business", "grow", "work")))
by_pres_mat <- dfm(recent_corpus, dictionary = my_dict, remove = stopwords("english"), stem = TRUE, remove_punct = TRUE)
by_pres_mat
myNgram <- tokens(my_corpus3) %>%
tokens_ngrams(n = 2) %>%
myNgram()
head(stopwords("en"), 20)
tokens_remove(tokens(my_corpus2), stopwords("english"), padding  = TRUE)
my_corpus3 <- tokens_remove(tokens(my_corpus2), stopwords("english"), padding  = TRUE)
my_corpus1 <- corpus(data_corpus_inaugural[1:5])
my_corpus2 <- corpus(data_corpus_inaugural[53:58])
my_corpus3 <- my_corpus1 + my_corpus2
### We'll use data from a few speeches to show the outputs
my_corpus <- corpus(data_corpus_inaugural[42:44])
#### Initial analysis, can take a look at uses of specific words from kwic in quanteda
kwic(my_corpus, pattern = "fight")
kwic(my_corpus, pattern = "Jesse Lecy")
kwic(my_corpus, pattern = "foe")
#### Preprocessing Steps, not necessarily needs to be done in this order ###
### Making lowercase, from base package
tolower(my_corpus)
my_corpus1 <- tolower(my_corpus)
###tokenize, using quanteda which allows us to also remove punctuation
tokens(my_corpus1, remove_punct = TRUE)
my_corpus2 <- tokens(my_corpus1, remove_punct = TRUE)
### We can also split speeches into sentences rather than keeping them whole using the tokens command
tokens(my_corpus1,   what = "sentence")
#### Removing stopwords using quanteda, padding adds space where a stopword is removed to help identify ngrams
head(stopwords("en"), 20)
tokens_remove(tokens(my_corpus2), stopwords("english"), padding  = TRUE)
my_corpus3 <- tokens_remove(tokens(my_corpus2), stopwords("english"), padding  = TRUE)
my_corpus <- corpus(data_corpus_inaugural[42:48])
#### Initial analysis, can take a look at uses of specific words from kwic in quanteda
kwic(my_corpus, pattern = "fight")
kwic(my_corpus, pattern = "Jesse Lecy")
kwic(my_corpus, pattern = "foe")
#### Preprocessing Steps, not necessarily needs to be done in this order ###
### Making lowercase, from base package
tolower(my_corpus)
my_corpus1 <- tolower(my_corpus)
###tokenize, using quanteda which allows us to also remove punctuation
tokens(my_corpus1, remove_punct = TRUE)
my_corpus2 <- tokens(my_corpus1, remove_punct = TRUE)
### We can also split speeches into sentences rather than keeping them whole using the tokens command
tokens(my_corpus1,   what = "sentence")
#### Removing stopwords using quanteda, padding adds space where a stopword is removed to help identify ngrams
head(stopwords("en"), 20)
tokens_remove(tokens(my_corpus2), stopwords("english"), padding  = TRUE)
my_corpus3 <- tokens_remove(tokens(my_corpus2), stopwords("english"), padding  = TRUE)
#### Ngrams takes multiple steps, first seeing the most common phrases
myNgram <- tokens(my_corpus3) %>%
tokens_ngrams(n = 2) %>%
myNgram()
topfeatures(myDfm)
myNgram <- tokens(my_corpus3) %>%
tokens_ngrams(n = 2) %>%
myDfm()
myDfm <- tokens(my_corpus1.3) %>%
tokens_ngrams(n = 2) %>%
dfm()
myDfm <- tokens(my_corpus3) %>%
tokens_ngrams(n = 2) %>%
dfm()
topfeatures(myDfm)
my_corpus1 <- corpus(data_corpus_inaugural[1:5])
my_corpus2 <- corpus(data_corpus_inaugural[53:58])
my_corpus3 <- my_corpus1 + my_corpus2
### We'll use data from a few speeches to show the outputs
my_corpus <- corpus(data_corpus_inaugural[42:48])
#### Initial analysis, can take a look at uses of specific words from kwic in quanteda
kwic(my_corpus, pattern = "fight")
kwic(my_corpus, pattern = "Jesse Lecy")
kwic(my_corpus, pattern = "foe")
#### Preprocessing Steps, not necessarily needs to be done in this order ###
### Making lowercase, from base package
tolower(my_corpus)
my_corpus1 <- tolower(my_corpus)
tokens(my_corpus1, remove_punct = TRUE)
my_corpus2 <- tokens(my_corpus1, remove_punct = TRUE)
### We can also split speeches into sentences rather than keeping them whole using the tokens command
tokens(my_corpus1,   what = "sentence")
#### Removing stopwords using quanteda, padding adds space where a stopword is removed to help identify ngrams
head(stopwords("en"), 20)
tokens_remove(tokens(my_corpus2), stopwords("english"), padding  = TRUE)
my_corpus3 <- tokens_remove(tokens(my_corpus2), stopwords("english"), padding  = TRUE)
#### Ngrams takes multiple steps
myDfm <- tokens(my_corpus3) %>%
tokens_ngrams(n = 2) %>%
dfm()
topfeatures(myDfm)
comp_toks <- tokens_compound(my_corpus3, pattern = phrase(c('vice president', 'almighty god*')))
comp_toks
topfeatures(my_corpus4)
my_corpus4 <- tokens_compound(my_corpus3, pattern = phrase(c('vice president', 'almighty god*')))
topfeatures(my_corpus4)
(my_corpus4)[6]
(my_corpus4)[6](20)
(my_corpus4)[6][20]
(my_corpus4)[6][,20]
(my_corpus4)[6][20,]
sort(table(my_corpus4), decreasing=T)
TermDocumentMatrix(my_corpus4)
install.packages("tm")
library(tm)
TermDocumentMatrix(my_corpus4)
TermDocumentMatrix(my_corpus4)
head(my_corpus4)
TermDocumentMatrix(my_corpus4)[6]
head(my_corpus4)[6]
kwic(my_corpus4, pattern =  'vice president')
kwic(my_corpus4, pattern =  'vice_president')
setwd("C:/Users/evanholm/Dropbox (ASU)/USC Mission Paper/Data and Analysis/github_files/machine_learning_mission_codes/DATA")
library(quanteda)
library(tm)
mission <- read.csv("MISSION.csv")
mission <- read.csv("MISSION.csv")
programs <- read.csv("PROGRAMS.csv")
names(programs)
programs <- programs[, c("NAME", "EIN", "TAXYR", "DESCRIPTION")]
miss.prog <- merge(mission, program, by=c("NAME", "EIN", "TAXYR"))
miss.prog <- merge(mission, programs, by=c("NAME", "EIN", "TAXYR"))
dups <- miss.prog[ duplicated(c("NAME", "EIN", "TAXYR")),]
dups <- miss.prog[ duplicated(c("EIN", "TAXYR")),]
miss.prog$unique <- paste(miss.prog$EIN), miss.prog$TAXYR
miss.prog$unique <- paste(miss.prog$EIN, miss.prog$TAXYR)
dups <- miss.prog[ duplicated(unique),]
dups <- miss.prog[ duplicated('unique'),]
mission <- merge(mission, programs, by=c("NAME", "EIN", "TAXYR"))
dups <- mission[ duplicated(unique),]
mission$unique <- paste(miss.prog$EIN, miss.prog$TAXYR)
dups <- mission[ duplicated(unique),]
dups <- mission[ duplicated(mission$unique),]
View(dups)
mission <- mission[!duplicated(mission[c('EIN', 'TAXYR', 'F9_03_PZ_MISSION')]),]
programs <- programs[, c("EIN", "TAXYR", "DESCRIPTION")]
programs <- programs[!duplicated(mission[c('EIN', 'TAXYR', 'DESCRIPTION')]),]
mp <- merge(mission, programs, by=c( "EIN", "TAXYR"))
mp$unique <- paste(mp$EIN, mp$TAXYR)
dups <- mp[ duplicated(mp$unique),]
mission <- read.csv("MISSION.csv")
programs <- read.csv("PROGRAMS.csv")
mission <- mission[!duplicated(mission[c('EIN', 'TAXYR', 'F9_03_PZ_MISSION')]),]
mission <- mission[!duplicated(mission[c('EIN', 'TAXYR')]),]
mission <- read.csv("MISSION.csv")
programs <- read.csv("PROGRAMS.csv")
mission <- mission[!duplicated(mission[c('EIN', 'TAXYR', 'F9_03_PZ_MISSION')]),]
programs <- programs[, c("EIN", "TAXYR", "DESCRIPTION")]
programs <- programs[!duplicated(mission[c('EIN', 'TAXYR', 'DESCRIPTION')]),]
mp <- merge(mission, programs, by=c( "EIN", "TAXYR"))
mission <- read.csv("MISSION.csv")
programs <- read.csv("PROGRAMS.csv")
mission <- mission[!duplicated(mission[c('EIN', 'TAXYR', 'F9_03_PZ_MISSION')]),]
programs <- programs[, c("EIN", "TAXYR", "DESCRIPTION")]
programs <- programs[!duplicated(programs[c('EIN', 'TAXYR', 'DESCRIPTION')]),]
mp <- merge(mission, programs, by=c( "EIN", "TAXYR"))
mp$text <- paste(mp$NAME, mp$F9_03_PZ_MISSION, mp$de)
mp$text <- paste(mp$NAME, mp$F9_03_PZ_MISSION, mp$DESCRIPTION)
mission <- read.csv("MISSION.csv")
programs <- read.csv("PROGRAMS.csv")
mission <- mission[!duplicated(mission[c('EIN', 'TAXYR', 'F9_03_PZ_MISSION')]),]
programs <- programs[, c("EIN", "TAXYR", "DESCRIPTION")]
programs <- programs[!duplicated(programs[c('EIN', 'TAXYR', 'DESCRIPTION')]),]
mp <- merge(mission, programs, by=c( "EIN", "TAXYR"))
mp$text <- paste(mp$NAME, mp$F9_03_PZ_MISSION, mp$DESCRIPTION)
mp.corp <- corpus(mp,
text_field = "text")
library(quanteda)
mp.corp <- corpus(mp,
text_field = "text")
install.packages('rlang')
library(quanteda)
library(tm)
library(corpus)
mp.corp <- corpus(mp,
text_field = "text")
summary(mp.corp)[1:10,]
mp.corp2 <- tolower(mp.corp)
mp.corp3 <- tokens(mp.corp2, remove_punct = TRUE)
mp.corp4 <- tokens_remove(tokens(mp.corp3), c(stopwords("english"), "nbsp"), padding  = F)
myNgram2 <- tokens(mission.corp4) %>%
tokens_ngrams(n = 2) %>%
dfm()
myNgram3 <- tokens(mission.corp4) %>%
tokens_ngrams(n = 3) %>%
dfm()
myNgram2miss.df <- textstat_frequency(myNgram2)
myNgram3miss.df <- textstat_frequency(myNgram3)
myNgram2 <- tokens(mp.corp4) %>%
tokens_ngrams(n = 2) %>%
dfm()
myNgram3 <- tokens(mp.corp4) %>%
tokens_ngrams(n = 3) %>%
dfm()
myNgram2miss.df <- textstat_frequency(myNgram2)
myNgram3miss.df <- textstat_frequency(myNgram3)
topfeatures(myNgram2)
topfeatures(myNgram3)
miss.ngrams <- rbind(myNgram2miss.df, myNgram3miss.df)
write.csv(prog.ngrams, "Full Ngrams")
write.csv(miss.ngrams, "Full Ngrams")
write.csv(miss.ngrams, "Full Ngrams.csv")
write.csv(miss.ngrams, "Full Ngrams.csv")
my_dict_prog <- dictionary(list(five01_c_3= c("501 c 3","section 501 c 3") ,
jesus_christ=c("jesus christ"),
high_school=c("high school"),
non_profit=c("non-profit", "non profit")
stem=c("science technology engineering math", "science technology engineering mathematics"),
steam=c("science technology engineering art math", "science technology engineering art mathematics")))
my_dict_place <- dictionary(list(ny_city=c("new york city"),
ny_state=c("new york state"),
ny=c("new york"),
sf=c("san francisco"),
san_diego=c("san diego"),
santa_barbara=c("santa barbara"),
new_hampshire=c("new hampshire"),
new_orleans=c("new orleans"),
san_antonio=c("san antonio"),
san_gabriel=c("san gabriel"),
santa_monica=c("santa monica"),
santa_clarita=c("santa clarita"),
los_angeles=c("los angeles"),
united_states = "united states")))
my_dict_prog <- dictionary(list(five01_c_3= c("501 c 3","section 501 c 3") ,
jesus_christ=c("jesus christ"),
high_school=c("high school"),
non_profit=c("non-profit", "non profit")
stem=c("science technology engineering math", "science technology engineering mathematics"),
steam=c("science technology engineering art math", "science technology engineering art mathematics"))
my_dict_prog <- dictionary(list(five01_c_3= c("501 c 3","section 501 c 3") ,
jesus_christ=c("jesus christ"),
high_school=c("high school"),
non_profit=c("non-profit", "non profit")
stem=c("science technology engineering math", "science technology engineering mathematics"),
steam=c("science technology engineering art math", "science technology engineering art mathematics")))
my_dict_prog <- dictionary(list(five01_c_3= c("501 c 3","section 501 c 3") ,
jesus_christ=c("jesus christ"),
high_school=c("high school"),
non_profit=c("non-profit", "non profit"),
stem=c("science technology engineering math", "science technology engineering mathematics"),
steam=c("science technology engineering art math", "science technology engineering art mathematics")))
my_dict_place <- dictionary(list(ny_city=c("new york city"),
ny_state=c("new york state"),
ny=c("new york"),
sf=c("san francisco"),
san_diego=c("san diego"),
santa_barbara=c("santa barbara"),
new_hampshire=c("new hampshire"),
new_orleans=c("new orleans"),
san_antonio=c("san antonio"),
san_gabriel=c("san gabriel"),
santa_monica=c("santa monica"),
santa_clarita=c("santa clarita"),
los_angeles=c("los angeles"),
united_states = "united states")))
my_dict_place <- dictionary(list(ny_city=c("new york city"),
ny_state=c("new york state"),
ny=c("new york"),
sf=c("san francisco"),
san_diego=c("san diego"),
santa_barbara=c("santa barbara"),
new_hampshire=c("new hampshire"),
new_orleans=c("new orleans"),
san_antonio=c("san antonio"),
san_gabriel=c("san gabriel"),
santa_monica=c("santa monica"),
santa_clarita=c("santa clarita"),
los_angeles=c("los angeles")
united_states = c("united states")))
my_dict_place <- dictionary(list(ny_city=c("new york city"),
ny_state=c("new york state"),
ny=c("new york"),
sf=c("san francisco"),
san_diego=c("san diego"),
santa_barbara=c("santa barbara"),
new_hampshire=c("new hampshire"),
new_orleans=c("new orleans"),
san_antonio=c("san antonio"),
san_gabriel=c("san gabriel"),
santa_monica=c("santa monica"),
santa_clarita=c("santa clarita"),
los_angeles=c("los angeles"),
united_states = c("united states")))
mp.corp5 <- tokens_compound(mp.corp4, pattern = my_dict_prog)
mp.corp6 <- tokens_compound(mp.corp5, pattern = my_dict_place)
mp.corp7 <- sapply(mp.corp6, paste, collapse=c(" ", "  "))
mission.dfm <- dfm(mp.corp7,
stem = T)
mission.dfm2 <- dfm_trim(mission.dfm, sparsity = )
mission.dfm
topfeatures(mission.dfm, 20)
mission.dfm.df <- convert(mission.dfm, to = "data.frame")
mission.corpus.df <- as.data.frame(mission.corp6)
colnames(mission.corpus.df) <- "Corpus"
mission.corpus.df <- as.data.frame(mp.corp7)
colnames(mission.corpus.df) <- "Corpus"
mission2 <- cbind(mp, mission.corpus.df)
mission3 <- cbind(mission2, mission.dfm.df)
write.csv(mission3, "Name Mission Program w Corpus and DFM.csv")
